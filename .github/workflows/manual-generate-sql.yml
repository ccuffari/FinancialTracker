# .github/workflows/generate-db-structure.yml
name: Generate Database Structure

on:
  workflow_dispatch:
    inputs:
      branch:
        description: "Branch da utilizzare"
        required: true
        default: "main"
        type: string
      python_version:
        description: "Versione Python"
        required: true
        default: "3.10"
        type: choice
        options:
          - "3.8"
          - "3.9"
          - "3.10"
          - "3.11"
          - "3.12"
      deploy_to_neon:
        description: "Deploy su PostgreSQL Neon (necessario per Provisioning/ETL)"
        required: true
        default: false
        type: boolean
      populate_data:
        description: "Popola tabelle con dati Excel (usato dall'ETL)"
        required: true
        default: true
        type: boolean
      run_data_modeling:
        description: "Esegui Data Modeling (genera SQL DDL)"
        required: true
        default: true
        type: boolean
      run_data_provisioning:
        description: "Esegui Data Provisioning (deploy dello schema su Neon)"
        required: true
        default: false
        type: boolean
      run_etl:
        description: "Esegui ETL (popolamento tabelle)"
        required: true
        default: false
        type: boolean

jobs:
  # -------------------------------------------------------
  # 1) Initialization & validation
  # -------------------------------------------------------
  initialization:
    name: Initialization & validation
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch }}
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Validate user selections (at least one of Modeling/Provisioning/ETL)
        run: |
          echo "Validating workflow inputs..."
          RM="${{ github.event.inputs.run_data_modeling }}"
          RP="${{ github.event.inputs.run_data_provisioning }}"
          RE="${{ github.event.inputs.run_etl }}"
          DEPLOY="${{ github.event.inputs.deploy_to_neon }}"

          echo " - run_data_modeling: $RM"
          echo " - run_data_provisioning: $RP"
          echo " - run_etl: $RE"
          echo " - deploy_to_neon: $DEPLOY"

          if [ "$RM" != "true" ] && [ "$RP" != "true" ] && [ "$RE" != "true" ]; then
            echo "âŒ Errore: almeno uno tra Data Modeling, Data Provisioning o ETL deve essere selezionato."
            exit 1
          fi

          if { [ "$RP" = "true" ] || [ "$RE" = "true" ]; } && [ "$DEPLOY" != "true" ]; then
            echo "âŒ Errore: Data Provisioning ed ETL richiedono 'deploy_to_neon' = true."
            exit 1
          fi

          echo "âœ… Validazione completata."

      - name: Quick check Excel file present?
        run: |
          if [ ! -f "data/financialTracker.xlsx" ]; then
            echo "âš ï¸ File data/financialTracker.xlsx non trovato in repo. Se Data Modeling/ETL richiedono il file, il job fallirÃ  piÃ¹ avanti."
          else
            echo "âœ… File Excel trovato"
            ls -la data/financialTracker.xlsx
          fi

      - name: Create output directory
        run: |
          mkdir -p sql/ddl
          echo "âœ… Directory sql/ddl creata (o giÃ  presente)"

  # -------------------------------------------------------
  # 2) Data Modeling (genera DDL) - opzionale
  # -------------------------------------------------------
  data_modeling:
    name: Data Modeling (generate DDL)
    runs-on: ubuntu-latest
    needs: initialization
    if: ${{ github.event.inputs.run_data_modeling == 'true' }}
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ github.event.inputs.python_version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('models/python/db_structure_generator/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r models/python/db_structure_generator/requirements.txt
          pip install psycopg2-binary

      - name: Verify Excel file exists (required for modeling)
        run: |
          if [ ! -f "data/financialTracker.xlsx" ]; then
            echo "âŒ File data/financialTracker.xlsx non trovato! (necessario per il Data Modeling)"
            exit 1
          fi
          echo "âœ… File Excel trovato"

      - name: Generate database structure
        run: |
          cd models/python/db_structure_generator
          python main.py

      - name: Verify generated SQL file
        run: |
          if [ ! -f "sql/ddl/financial_tracker_ddl.sql" ]; then
            echo "âŒ File SQL non generato!"
            exit 1
          fi
          echo "âœ… File SQL generato con successo"
          echo "ðŸ“Š Dimensione file: $(du -h sql/ddl/financial_tracker_ddl.sql | cut -f1)"
          echo "ðŸ“„ Prime 10 righe del file:"
          head -10 sql/ddl/financial_tracker_ddl.sql

      - name: Upload SQL artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-ddl-${{ github.event.inputs.branch }}-py${{ github.event.inputs.python_version }}
          path: sql/ddl/financial_tracker_ddl.sql
          retention-days: 30

  # -------------------------------------------------------
  # 3) Data Provisioning (deploy schema su Neon) - opzionale
  # -------------------------------------------------------
  data_provisioning:
    name: Data Provisioning (deploy schema to Neon)
    runs-on: ubuntu-latest
    needs: initialization
    if: ${{ github.event.inputs.run_data_provisioning == 'true' }}
    permissions:
      contents: write
    env:
      PGHOST: ${{ secrets.PGHOST }}
      PGDATABASE: ${{ secrets.PGDATABASE }}
      PGUSER: ${{ secrets.PGUSER }}
      PGPASSWORD: ${{ secrets.PGPASSWORD }}
      PGSSLMODE: ${{ secrets.PGSSLMODE }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch }}

      - name: Try download SQL artifact from this run (if Data Modeling ran)
        uses: actions/download-artifact@v4
        with:
          name: database-ddl-${{ github.event.inputs.branch }}-py${{ github.event.inputs.python_version }}
          path: sql/ddl
        continue-on-error: true

      - name: Locate SQL file (artifact or repo)
        run: |
          if [ -f "sql/ddl/financial_tracker_ddl.sql" ]; then
            echo "âœ… SQL file found at sql/ddl/financial_tracker_ddl.sql"
          else
            echo "âŒ Nessun file SQL disponibile per il deploy."
            echo "   - Esegui prima il Data Modeling in questa workflow (run_data_modeling=true) oppure assicurati che sql/ddl/financial_tracker_ddl.sql esista nel branch selezionato"
            exit 1
          fi

      - name: Set up Python (for psycopg2)
        uses: actions/setup-python@v5
        with:
          python-version: ${{ github.event.inputs.python_version }}

      - name: Install psycopg2
        run: |
          python -m pip install --upgrade pip
          pip install psycopg2-binary

      - name: Create deploy script
        run: |
          cat > deploy_ddl.py << 'EOF'
          import psycopg2, os, sys
          try:
              conn = psycopg2.connect(
                  host=os.environ['PGHOST'],
                  database=os.environ['PGDATABASE'],
                  user=os.environ['PGUSER'],
                  password=os.environ['PGPASSWORD'],
                  sslmode=os.environ.get('PGSSLMODE', 'require')
              )
              with conn.cursor() as cursor:
                  print('ðŸ“– Lettura file SQL...')
                  with open('sql/ddl/financial_tracker_ddl.sql', 'r', encoding='utf-8') as f:
                      sql_content = f.read()
                  print('ðŸ”„ Esecuzione DDL sul database...')
                  cursor.execute(sql_content)
                  conn.commit()
                  cursor.execute("""
                      SELECT schemaname, tablename 
                      FROM pg_tables 
                      WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
                      ORDER BY schemaname, tablename;
                  """)
                  tables = cursor.fetchall()
                  print('ðŸ“Š Tabelle create (count):', len(tables))
                  for schema, table in tables:
                      print(f'   - {schema}.{table}')
              conn.close()
              print('ðŸŽ‰ DDL Deploy completato con successo!')
          except Exception as e:
              print('âŒ Errore durante il DDL deploy:', e)
              sys.exit(1)
          EOF

      - name: Deploy DDL to PostgreSQL Neon
        run: |
          echo "ðŸš€ Inizio deploy su PostgreSQL Neon..."
          python deploy_ddl.py

  # -------------------------------------------------------
  # 4) ETL (popolamento tabelle) - opzionale
  # -------------------------------------------------------
  etl:
    name: ETL (populate tables)
    runs-on: ubuntu-latest
    needs: initialization
    if: ${{ github.event.inputs.run_etl == 'true' }}
    permissions:
      contents: write
    env:
      PGHOST: ${{ secrets.PGHOST }}
      PGDATABASE: ${{ secrets.PGDATABASE }}
      PGUSER: ${{ secrets.PGUSER }}
      PGPASSWORD: ${{ secrets.PGPASSWORD }}
      PGSSLMODE: ${{ secrets.PGSSLMODE }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch }}

      - name: Try download SQL artifact from this run (optional)
        uses: actions/download-artifact@v4
        with:
          name: database-ddl-${{ github.event.inputs.branch }}-py${{ github.event.inputs.python_version }}
          path: sql/ddl
        continue-on-error: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ github.event.inputs.python_version }}

      - name: Install ETL dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r models/python/db_structure_generator/requirements.txt || true
          pip install psycopg2-binary openpyxl pandas || true

      - name: Create ETL script
        run: |
          cd models/python/db_structure_generator
          cat > run_etl.py << 'EOF'
          import os, sys
          try:
              # Project-specific modules (config, extractor, etl) are expected in repo
              from config import EXCEL_FILE
              from extractor import extract_sheets
              from etl import load_dataframe_to_table
              import psycopg2

              mapping = extract_sheets(EXCEL_FILE)

              # If etl module exposes get_connection, override with environment-based connection
              try:
                  import etl as etl_module
                  def neon_conn():
                      return psycopg2.connect(
                          host=os.environ['PGHOST'],
                          database=os.environ['PGDATABASE'],
                          user=os.environ['PGUSER'],
                          password=os.environ['PGPASSWORD'],
                          sslmode=os.environ.get('PGSSLMODE', 'require')
                      )
                  if hasattr(etl_module, 'get_connection'):
                      etl_module.get_connection = neon_conn
              except Exception:
                  pass

              total = 0
              for (schema, table), df in mapping.items():
                  print(f'Caricamento {schema}.{table} - {len(df)} righe')
                  load_dataframe_to_table(schema, table, df)
                  total += len(df)
              print("ETL completato, righe caricate:", total)
          except Exception as e:
              print("âŒ Errore ETL:", e)
              sys.exit(1)
          EOF

      - name: Run ETL script to populate tables
        run: |
          echo "ðŸ“Š Inizio ETL (popolamento tabelle)..."
          cd models/python/db_structure_generator
          python run_etl.py

  # -------------------------------------------------------
  # 5) Closure & summary (non dipende da job condizionali)
  # -------------------------------------------------------
  closure:
    name: Closure & summary
    runs-on: ubuntu-latest
    needs: initialization
    permissions:
      contents: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch }}

      - name: Display run summary
        run: |
          echo "ðŸŽ‰ Workflow summary"
          echo " - Branch: ${{ github.event.inputs.branch }}"
          echo " - Python: ${{ github.event.inputs.python_version }}"
          echo " - run_data_modeling: ${{ github.event.inputs.run_data_modeling }}"
          echo " - run_data_provisioning: ${{ github.event.inputs.run_data_provisioning }}"
          echo " - run_etl: ${{ github.event.inputs.run_etl }}"
          echo " - deploy_to_neon: ${{ github.event.inputs.deploy_to_neon }}"
          echo " - populate_data: ${{ github.event.inputs.populate_data }}"

      - name: Optional Commit generated SQL back to branch (main/develop)
        if: (github.event.inputs.branch == 'main' || github.event.inputs.branch == 'develop') && github.event.inputs.run_data_modeling == 'true'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          if [ -n "$(git status --porcelain sql/ddl/)" ]; then
            git add sql/ddl/financial_tracker_ddl.sql || true
            git commit -m "Auto-generated DDL: financial_tracker_ddl.sql [branch:${{ github.event.inputs.branch }} py:${{ github.event.inputs.python_version }}]" || true
            git push || true
            echo "âœ… File committato e pushato automaticamente"
          else
            echo "â„¹ï¸ Nessuna modifica al file SQL da committare"
          fi
