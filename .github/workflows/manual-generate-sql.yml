# .github/workflows/generate-db-structure.yml
name: Generate Database Structure

on:
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch da utilizzare'
        required: true
        default: 'main'
        type: string
      python_version:
        description: 'Versione Python'
        required: true
        default: '3.10'
        type: choice
        options:
          - '3.8'
          - '3.9'
          - '3.10'
          - '3.11'
          - '3.12'
      deploy_to_neon:
        description: 'Deploy su PostgreSQL Neon'
        required: true
        default: false
        type: boolean
      populate_data:
        description: 'Popola tabelle con dati Excel'
        required: true
        default: true
        type: boolean

jobs:
  generate-ddl:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Permesso per scrivere nel repository
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.inputs.branch }}
        token: ${{ secrets.GITHUB_TOKEN }}  # Usa il token per i permessi
    
    - name: Set up Python ${{ github.event.inputs.python_version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ github.event.inputs.python_version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('models/python/db_structure_generator/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r models/python/db_structure_generator/requirements.txt
        pip install psycopg2-binary  # Per la connessione PostgreSQL
    
    - name: Verify Excel file exists
      run: |
        if [ ! -f "data/financialTracker.xlsx" ]; then
          echo "‚ùå File data/financialTracker.xlsx non trovato!"
          exit 1
        fi
        echo "‚úÖ File Excel trovato"
        ls -la data/financialTracker.xlsx
    
    - name: Create output directory
      run: |
        mkdir -p sql/ddl
        echo "‚úÖ Directory sql/ddl creata"
    
    - name: Generate database structure
      run: |
        cd models/python/db_structure_generator
        python main.py
    
    - name: Verify generated SQL file
      run: |
        if [ ! -f "sql/ddl/financial_tracker_ddl.sql" ]; then
          echo "‚ùå File SQL non generato!"
          exit 1
        fi
        echo "‚úÖ File SQL generato con successo"
        echo "üìä Dimensione file: $(du -h sql/ddl/financial_tracker_ddl.sql | cut -f1)"
        echo "üìÑ Prime 20 righe del file:"
        head -20 sql/ddl/financial_tracker_ddl.sql
    
    - name: Upload SQL artifact
      uses: actions/upload-artifact@v4
      with:
        name: database-ddl-${{ github.event.inputs.branch }}-py${{ github.event.inputs.python_version }}
        path: sql/ddl/financial_tracker_ddl.sql
        retention-days: 30
    
    # Deploy su PostgreSQL Neon (opzionale)
    - name: Deploy to PostgreSQL Neon
      if: github.event.inputs.deploy_to_neon == 'true'
      env:
        PGHOST: ${{ secrets.PGHOST }}
        PGDATABASE: ${{ secrets.PGDATABASE }}
        PGUSER: ${{ secrets.PGUSER }}
        PGPASSWORD: ${{ secrets.PGPASSWORD }}
        PGSSLMODE: ${{ secrets.PGSSLMODE }}
      run: |
        echo "üöÄ Inizio deploy su PostgreSQL Neon..."
        
        # Test connessione database
        python -c "
        import psycopg2
        import os
        import sys
        
        try:
            conn = psycopg2.connect(
                host=os.environ['PGHOST'],
                database=os.environ['PGDATABASE'],
                user=os.environ['PGUSER'],
                password=os.environ['PGPASSWORD'],
                sslmode=os.environ['PGSSLMODE']
            )
            print('‚úÖ Connessione al database riuscita')
            
            # Esegui il DDL
            with conn.cursor() as cursor:
                print('üìñ Lettura file SQL...')
                with open('sql/ddl/financial_tracker_ddl.sql', 'r', encoding='utf-8') as f:
                    sql_content = f.read()
                
                print('üîÑ Esecuzione DDL sul database...')
                cursor.execute(sql_content)
                conn.commit()
                print('‚úÖ DDL eseguito con successo!')
                
                # Verifica tabelle create
                cursor.execute('''
                    SELECT schemaname, tablename 
                    FROM pg_tables 
                    WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
                    ORDER BY schemaname, tablename;
                ''')
                tables = cursor.fetchall()
                
                print(f'üìä Tabelle create ({len(tables)}):')
                for schema, table in tables:
                    print(f'   - {schema}.{table}')
            
            conn.close()
            print('üéâ DDL Deploy completato con successo!')
            
        except Exception as e:
            print(f'‚ùå Errore durante il DDL deploy: {str(e)}')
            sys.exit(1)
        "
    
    # NUOVO: Popola tabelle con dati Excel
    - name: Populate tables with Excel data
      if: github.event.inputs.deploy_to_neon == 'true' && github.event.inputs.populate_data == 'true'
      env:
        PGHOST: ${{ secrets.PGHOST }}
        PGDATABASE: ${{ secrets.PGDATABASE }}
        PGUSER: ${{ secrets.PGUSER }}
        PGPASSWORD: ${{ secrets.PGPASSWORD }}
        PGSSLMODE: ${{ secrets.PGSSLMODE }}
      run: |
        echo "üìä Inizio popolamento tabelle con dati Excel..."
        
        cd models/python/db_structure_generator
        
        # Crea un script ETL dedicato che usa le variabili d'ambiente per la connessione Neon
        python -c "
        import os
        import sys
        import logging
        from config import EXCEL_FILE
        from extractor import extract_sheets
        from etl import load_dataframe_to_table
        import psycopg2
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger('etl_neon')
        
        try:
            # Connessione al database Neon usando variabili d'ambiente
            conn = psycopg2.connect(
                host=os.environ['PGHOST'],
                database=os.environ['PGDATABASE'],
                user=os.environ['PGUSER'],
                password=os.environ['PGPASSWORD'],
                sslmode=os.environ['PGSSLMODE']
            )
            print('‚úÖ Connessione database ETL riuscita')
            
            # Estrai dati Excel
            print('üìñ Lettura file Excel per ETL...')
            mapping = extract_sheets(EXCEL_FILE)
            
            # Gestisci tabella dates
            dates_df = None
            dates_schema = 'dates'
            dates_table = 'dates'
            dates_pk_col = 'id'
            dates_date_col = None
            
            if (dates_schema, dates_table) in mapping:
                dates_df = mapping.pop((dates_schema, dates_table))
                # Rileva colonne come nel main.py
                for c in dates_df.columns:
                    if c.lower() == 'id' or c.lower().endswith('_id'):
                        dates_pk_col = c
                        break
                for c in dates_df.columns:
                    if 'date' in c.lower():
                        dates_date_col = c
                        break
            
            # Modifica temporaneamente la configurazione del database in etl.py
            # (questo √® un workaround - idealmente dovresti passare la connessione)
            import etl
            
            # Backup configurazione originale
            original_config = {}
            if hasattr(etl, 'get_connection'):
                # Sovrascrivi la funzione di connessione per usare Neon
                def get_neon_connection():
                    return psycopg2.connect(
                        host=os.environ['PGHOST'],
                        database=os.environ['PGDATABASE'],
                        user=os.environ['PGUSER'],
                        password=os.environ['PGPASSWORD'],
                        sslmode=os.environ['PGSSLMODE']
                    )
                etl.get_connection = get_neon_connection
            
            # Esegui ETL per ogni tabella
            total_rows = 0
            for (schema, table), df in mapping.items():
                try:
                    print(f'üìä Caricamento dati per {schema}.{table} ({len(df)} righe)...')
                    load_dataframe_to_table(schema, table, df, dates_pk_col=dates_pk_col, dates_date_col=dates_date_col)
                    total_rows += len(df)
                    print(f'‚úÖ Dati caricati per {schema}.{table}')
                except Exception as e:
                    print(f'‚ùå Errore ETL per {schema}.{table}: {str(e)}')
                    raise
            
            conn.close()
            print(f'üéâ ETL completato! Totale righe caricate: {total_rows}')
            
        except Exception as e:
            print(f'‚ùå Errore durante ETL: {str(e)}')
            sys.exit(1)
        "
    
    # Verifica finale dati
    - name: Verify data population
      if: github.event.inputs.deploy_to_neon == 'true' && github.event.inputs.populate_data == 'true'
      env:
        PGHOST: ${{ secrets.PGHOST }}
        PGDATABASE: ${{ secrets.PGDATABASE }}
        PGUSER: ${{ secrets.PGUSER }}
        PGPASSWORD: ${{ secrets.PGPASSWORD }}
        PGSSLMODE: ${{ secrets.PGSSLMODE }}
      run: |
        echo "üîç Verifica popolamento dati..."
        
        python -c "
        import psycopg2
        import os
        
        conn = psycopg2.connect(
            host=os.environ['PGHOST'],
            database=os.environ['PGDATABASE'],
            user=os.environ['PGUSER'],
            password=os.environ['PGPASSWORD'],
            sslmode=os.environ['PGSSLMODE']
        )
        
        with conn.cursor() as cursor:
            # Conta righe per ogni tabella
            cursor.execute('''
                SELECT schemaname, tablename 
                FROM pg_tables 
                WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
                ORDER BY schemaname, tablename;
            ''')
            tables = cursor.fetchall()
            
            total_rows = 0
            print('üìä Righe per tabella:')
            for schema, table in tables:
                try:
                    cursor.execute(f'SELECT COUNT(*) FROM {schema}.{table};')
                    count = cursor.fetchone()[0]
                    total_rows += count
                    print(f'   - {schema}.{table}: {count} righe')
                except Exception as e:
                    print(f'   - {schema}.{table}: Errore conteggio ({str(e)})')
            
            print(f'üìà Totale righe nel database: {total_rows}')
        
        conn.close()
        "
    
    - name: Database deployment summary
      if: github.event.inputs.deploy_to_neon == 'true'
      run: |
        echo "üéâ Deploy su Neon completato!"
        echo "üåê Host: ${{ secrets.PGHOST }}"
        echo "üóÑÔ∏è Database: ${{ secrets.PGDATABASE }}"
        echo "üë§ User: ${{ secrets.PGUSER }}"
        echo "üîí SSL: Abilitato"
        
        if [ "${{ github.event.inputs.populate_data }}" == "true" ]; then
          echo "üìä Popolamento dati: Abilitato"
        else
          echo "üìä Popolamento dati: Solo struttura"
        fi
    
    - name: Display generation summary
      run: |
        echo "üéâ Generazione completata con successo!"
        echo "üìÅ Branch utilizzato: ${{ github.event.inputs.branch }}"
        echo "üêç Python versione: ${{ github.event.inputs.python_version }}"
        echo "üìù File generato: sql/ddl/financial_tracker_ddl.sql"
        echo "üíæ Artifact disponibile per il download per 30 giorni"
        
        if [ "${{ github.event.inputs.deploy_to_neon }}" == "true" ]; then
          echo "üöÄ Deploy su Neon: Abilitato"
          if [ "${{ github.event.inputs.populate_data }}" == "true" ]; then
            echo "üìä Popolamento dati: Completato"
          else
            echo "üìä Popolamento dati: Solo struttura"
          fi
        else
          echo "üöÄ Deploy su Neon: Disabilitato"
        fi
        
        # Statistiche del file
        if [ -f "sql/ddl/financial_tracker_ddl.sql" ]; then
          echo ""
          echo "üìä Statistiche file generato:"
          echo "   - Righe: $(wc -l < sql/ddl/financial_tracker_ddl.sql)"
          echo "   - Caratteri: $(wc -c < sql/ddl/financial_tracker_ddl.sql)"
          echo "   - Dimensione: $(du -h sql/ddl/financial_tracker_ddl.sql | cut -f1)"
        fi
    
    # Opzionale: Commit automatico del file generato
    - name: Commit generated SQL (optional)
      if: github.event.inputs.branch == 'main' || github.event.inputs.branch == 'develop'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        if [ -n "$(git status --porcelain sql/ddl/)" ]; then
          git add sql/ddl/financial_tracker_ddl.sql
          git commit -m "ü§ñ Auto-generated database DDL from financialTracker.xlsx
          
          - Branch: ${{ github.event.inputs.branch }}
          - Python: ${{ github.event.inputs.python_version }}
          - Generated: $(date '+%Y-%m-%d %H:%M:%S UTC')"
          
          git push
          echo "‚úÖ File committato e pushato automaticamente"
        else
          echo "‚ÑπÔ∏è Nessuna modifica al file SQL da committare"
        fi
